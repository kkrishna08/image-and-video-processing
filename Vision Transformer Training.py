# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vJ3W-rHnB48XrfI8N_tPpAeVuPBqDd0S
"""

!pip install transformers datasets torch torchvision tqdm matplotlib

import torch
from torch.utils.data import DataLoader
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import torch.optim as optim
import torch.nn as nn
from tqdm import tqdm

# Load custom VisionTransformer from the provided file
from model import VisionTransformer

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor()
])

# Load CIFAR-10 Dataset
train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)
test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

label_names = train_dataset.classes
print(f"Labels: {label_names}")

# Initialize Custom Vision Transformer for CIFAR-10
model = VisionTransformer(
    n_channels=3,          # CIFAR-10 images have 3 color channels
    embed_dim=768,         # Embedding dimension, similar to ViT-B/16
    n_layers=12,           # Number of encoder layers
    n_attention_heads=12,  # Attention heads, as per ViT-B/16
    forward_mul=4,         # MLP scaling factor in encoder blocks
    image_size=224,        # Resized image size (224x224 for ViT)
    patch_size=16,         # Patch size (similar to ViT-B/16)
    n_classes=10,          # CIFAR-10 has 10 classes
    dropout=0.1            # Dropout rate
)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

!pip install torchinfo
from torchinfo import summary

# Assuming 'model' is your Vision Transformer model
summary(model, input_size=(32, 3, 224, 224))  # (batch_size, channels, height, width)

def train_model(model, train_loader, criterion, optimizer, device, epochs=5):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        correct = 0
        total = 0
        for images, labels in tqdm(train_loader):
            images, labels = images.to(device), labels.to(device)
            # Forward pass
            outputs = model(images)
            loss = criterion(outputs, labels)
            # Backward pass and optimization
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            # Calculate accuracy
            total_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
        print(f"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}, "
              f"Accuracy: {100 * correct / total:.2f}%")

def evaluate_model(model, test_loader, device):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    print(f'Test Accuracy: {100 * correct / total:.2f}%')

train_model(model, train_loader, criterion, optimizer, device, epochs=5)
evaluate_model(model, test_loader, device)

